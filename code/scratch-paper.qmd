---
title: "Untitled"
format: 
  html: 
    toc: true
    toc-location: left
    code-fold: true
    theme: yeti
editor: visual
execute: 
  message: false
  warning: false
---

```{r libraries}
library(tidyverse)
library(here)
library(janitor)
library(ggeffects)
library(performance)
library(naniar) # or equivalent
library(flextable) # or equivalent
library(car)
library(broom)
# would be nice to have
library(corrplot)
library(AICcmodavg)
library(GGally)
library(MuMIn)
```

Read in the data:

```{r reading-data}
plant <- read_csv(here("data", "knb-lter-hfr.109.18 (1)", "hf109-01-sarracenia.csv")) %>% 
  # make column names cleaner
  clean_names() %>% 
  # selecting columns of interest 
  select(totmass, species, feedlevel, sla, chlorophyll, amass, num_lvs, num_phylls)
```


Visualize the missing data:

```{r missing-data-visualization}
gg_miss_var(plant)
```
**Figure 1: Missing Data.** Number of missing data entries on the x-axis and number of variables on the y-axis. Lines correspond to number of missing data values for each variable. 



Subsetting data by dropping NAs:

```{r subset-drop-NA}
plant_subset <- plant %>% 
  drop_na(sla, chlorophyll, amass, num_lvs, num_phylls)
```

Create a correlation plot:

(example writing) To determine the relationships between numerical variables in our dataset, we calculated Pearson's r and visually represented correlation using a correlation plot. 

```{r correlation-plot}
# calculate Pearson's r for numerical values only
plant_cor <- plant_subset %>% 
  select(feedlevel:num_phylls) %>% 
  # diagonals show correlation between variables and themselves = 1
  cor(method = "pearson")
  
# creating a correlation plot (visual representation of correlation matrix); larger shapes mean greater correlation; color represents direction
corrplot(plant_cor,
         # change the shape of what's in the cells
         method = "ellipse", 
         # add correlation coefficients into plot
         addCoef.col = "black"
         )  
```

Create a plot of each variable compared against the others

```{r pairs-plot}
plant_subset %>% 
  select(species:num_phylls) %>% 
  ggpairs()
```

Starting regression here:

To determine how species and physiological characteristics predict biomass, we fit multiple linear models. 

```{r null-and-full-models}
# null = no predictors in model
null <- lm(totmass ~ 1, data = plant_subset)
# full = all potential predictors in model
full <- lm(totmass ~ species + feedlevel + sla + chlorophyll + amass + num_lvs + num_phylls, data = plant_subset)
```

We visually assess normality and homoskedasticity of residuals using diagnostic plots for the full model:

```{r full-diagnostics}
par(mfrow = c(2, 2))
plot(full)
```

normality: no! check_normality function said residuals were not normal
homoskedasticity: residuals vs fitted shows a cone shaped distribution of residuals with the data being more clumped at the beginning before becoming wider across the x-axis; therefore the residuals are heteroskedastic; check_heteroscedasticity confirmed this as well

We also tested for normality using the Shapiro-Wilk test (null hypothesis: variable of interest (i.e residuals) are normally distributed). 

We tested for homoskedasticity using the Breusch-Pagan test (null hypothesis: variable of interest has constant variance).

```{r}
# in writing make sure to say what test is being done for check_normality
check_normality(full)
check_heteroscedasticity(full)
```

```{r model-logs}
# log function = natural log
# transforming data into natural log form to create normality and homoscedasticity
# when transforming, you're only concerned with the response variable, not the predictors
null_log <- lm(log(totmass) ~ 1, data = plant_subset)
full_log <- lm(log(totmass) ~ species + feedlevel + sla + chlorophyll + amass + num_lvs + num_phylls, data = plant_subset)

# assumptions of linear regression are met using log transformation
plot(full_log)
check_normality(full_log)
check_heteroscedasticity(full_log)
```

Evaluate multicollinearity:

```{r calculate-vif}
car::vif(full_log)
```

We evaluated multicollinearity by calculating generalized variance inflation factor and determined that there is no multicollinearity because the GVIF values were are less than 5, meaning none of the predictors were inflating the R^2 value.

try some more models:

addressing the question: what set of predictor variables best explains the response?
-maximizes variance but minimizes complexity

```{r}
model2_log <- lm(log(totmass) ~ species, data = plant_subset)
```

check assumptions for model 2:
```{r}
plot(model2_log)

check_normality(model2_log)
# residuals are normally distributed
check_heteroscedasticity(model2_log)
# variances appear homoscedastic
```

model 3!
```{r}
model3_log <- lm(log(totmass) ~ chlorophyll, data = plant_subset)
```

check assumptions for model 3:
```{r}
plot(model3_log)

check_normality(model3_log)
# non normality of residuals detected
check_heteroscedasticity(model3_log)
# error variance appears homoscedastic
```

model 4:
```{r}
model4_log <- lm(log(totmass) ~ feedlevel, data = plant_subset)
```

check assumptions for model 4:
```{r}
plot(model4_log)

check_normality(model4_log)
# non normality of residuals detected
check_heteroscedasticity(model4_log)
# variance appears homoscedastic
```


compare models using Akaike's Information cirterion (AIC) values:
- looks for simplest model that explains the most variance
- compromise between complexity of model and how well model predicts the response

```{r}
AICc(full_log)
AICc(model2_log)
AICc(null_log)
AICc(model3_log)
AICc(model4_log)

MuMIn::AICc(full_log, model2_log, null_log, model3_log, model4_log)
MuMIn::model.sel(full_log, model2_log, null_log, model3_log, model4_log)
# full_log has lowest AIC -> best predictor model
```

we compared models using AIC and chose the model with the lowest value, which was full_log.

# Results

We found that the _____ model including __ ___ __ predictors best predicted ____ (model summary like f-statistic, sample size, DF, etc). 

```{r}
summary(full_log)

table <- tidy(full_log, conf.int = TRUE) %>% 
  # change the p-value numbers if they're really small
  # change the estimates, stan error, and t-statistics to round to __ digits
  # using mutate
  # make it into a flextable
  flextable() %>% 
  # fit to the viewer
  autofit()

table
```

use ggpredict() to backtransform estimates
```{r}
model_pred <- ggpredict(full_log, terms = "species", back.transform = TRUE)

plot(model_pred, add.data = TRUE)
# jittered points are the original data, bars = confidence inteval

plot(ggpredict(full_log, terms = "chlorophyll", back.transform = TRUE), add.data = TRUE)

plot(ggpredict(full_log, terms = "sla", back.transform = TRUE), add.data = TRUE)

model_pred
# report results on scale of original variable, not log transformation; must be transparent that a log transformation was conducted and why you did it; make sure to emphasize results are on original scale
# all else held constant (constant = adjusted for values), the predicted biomass for each species are ("predicted" within CI)
```



https://bianca-berron.github.io/ENVS-193DS_homework-05/code/scratch-paper.html 



